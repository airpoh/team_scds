{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7b74791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef4a4b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset successfully loaded and combined.\n",
      "Total rows and columns: (4725012, 10)\n",
      "Column names: ['kind', 'commentId', 'channelId', 'videoId', 'authorId', 'textOriginal', 'parentCommentId', 'likeCount', 'publishedAt', 'updatedAt']\n",
      "\n",
      "First 3 sample rows:\n",
      "              kind  commentId  channelId  videoId  authorId  \\\n",
      "0  youtube#comment    1781382      14492    74288   2032536   \n",
      "1  youtube#comment     289571      14727    79618   3043229   \n",
      "2  youtube#comment     569077       3314    51826    917006   \n",
      "\n",
      "                                        textOriginal  parentCommentId  \\\n",
      "0  PLEASE LESBIAN FLAG I BEG YOU \\n\\nYou would ro...              NaN   \n",
      "1   Apply mashed potato juice and mixed it with curd        3198066.0   \n",
      "2                         69 missed calls from mars👽              NaN   \n",
      "\n",
      "   likeCount                publishedAt                  updatedAt  \n",
      "0          0  2023-08-15 21:48:52+00:00  2023-08-15 21:48:52+00:00  \n",
      "1          0  2023-10-02 13:08:22+00:00  2023-10-02 13:08:22+00:00  \n",
      "2          0  2024-05-31 12:03:12+00:00  2024-05-31 12:03:12+00:00  \n"
     ]
    }
   ],
   "source": [
    "# File names\n",
    "files = [\n",
    "    \"comments1.csv\",\n",
    "    \"comments2.csv\",\n",
    "    \"comments3.csv\",\n",
    "    \"comments4.csv\",\n",
    "    \"comments5.csv\"\n",
    "]\n",
    "\n",
    "# Read and merge into one DataFrame\n",
    "dataframes = [pd.read_csv(f) for f in files]\n",
    "df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Display dataset information\n",
    "print(\"Dataset successfully loaded and combined.\")\n",
    "print(f\"Total rows and columns: {df.shape}\")\n",
    "print(f\"Column names: {df.columns.tolist()}\")\n",
    "print(\"\\nFirst 3 sample rows:\")\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74a82182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining rows after dropping missing comments: 4724755\n",
      "\n",
      "Sample cleaned comments:\n",
      "                                        textOriginal  \\\n",
      "0  PLEASE LESBIAN FLAG I BEG YOU \\n\\nYou would ro...   \n",
      "1   Apply mashed potato juice and mixed it with curd   \n",
      "2                         69 missed calls from mars👽   \n",
      "3                                               Baaa   \n",
      "4    you look like raven from phenomena raven no cap   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  please lesbian flag i beg you \\n\\nyou would ro...  \n",
      "1   apply mashed potato juice and mixed it with curd  \n",
      "2                            missed calls from mars👽  \n",
      "3                                               baaa  \n",
      "4    you look like raven from phenomena raven no cap  \n"
     ]
    }
   ],
   "source": [
    "# Part 2: Data Preprocessing\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Step 1: Keep only relevant column for spam detection\n",
    "comments = df[['textOriginal']].copy()\n",
    "\n",
    "# Step 2: Drop rows with missing text\n",
    "comments.dropna(subset=['textOriginal'], inplace=True)\n",
    "print(f\"Remaining rows after dropping missing comments: {len(comments)}\")\n",
    "\n",
    "# Step 3: Define a function to clean text\n",
    "def clean_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    # Remove mentions (@username) and hashtags\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove extra whitespace\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# Step 4: Apply cleaning\n",
    "comments['cleaned_text'] = comments['textOriginal'].apply(clean_text)\n",
    "\n",
    "# Step 5: Preview\n",
    "print(\"\\nSample cleaned comments:\")\n",
    "print(comments[['textOriginal', 'cleaned_text']].head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edde5453",
   "metadata": {},
   "source": [
    "Train Spam Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c9476c",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"Youtube-Spam-Dataset.csv\")\n",
    "\n",
    "# Check first rows\n",
    "print(df.head())\n",
    "\n",
    "# Check class distribution (spam vs not spam)\n",
    "print(df['CLASS'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85620e2",
   "metadata": {},
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)  # remove URLs\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text)  # remove punctuation/numbers\n",
    "    words = text.split()\n",
    "    words = [stemmer.stem(w) for w in words if w not in stop_words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "df[\"cleaned\"] = df[\"CONTENT\"].apply(clean_text)\n",
    "print(df[[\"CONTENT\", \"cleaned\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d304441",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8801020408163265\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.81      0.86       176\n",
      "           1       0.86      0.94      0.90       216\n",
      "\n",
      "    accuracy                           0.88       392\n",
      "   macro avg       0.89      0.87      0.88       392\n",
      "weighted avg       0.88      0.88      0.88       392\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Load Spam Dataset\n",
    "# -----------------------------\n",
    "df = pd.read_csv(\"Youtube-Spam-Dataset.csv\")\n",
    "\n",
    "# Preprocess text\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)  # remove URLs\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text)  # remove punctuation/numbers\n",
    "    words = text.split()\n",
    "    words = [stemmer.stem(w) for w in words if w not in stop_words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "df[\"cleaned\"] = df[\"CONTENT\"].apply(clean_text)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Vectorize + Train Random Forest\n",
    "# -----------------------------\n",
    "X = df[\"cleaned\"]\n",
    "y = df[\"CLASS\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_vec = vectorizer.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.2, random_state=42)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = rf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Save Model + Vectorizer\n",
    "# -----------------------------\n",
    "pickle.dump(rf, open(\"rf_model.pkl\", \"wb\"))\n",
    "pickle.dump(vectorizer, open(\"vectorizer.pkl\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6be0ba91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        textOriginal  \\\n",
      "0  PLEASE LESBIAN FLAG I BEG YOU \\n\\nYou would ro...   \n",
      "1   Apply mashed potato juice and mixed it with curd   \n",
      "2                         69 missed calls from mars👽   \n",
      "3                                               Baaa   \n",
      "4    you look like raven from phenomena raven no cap   \n",
      "5                                           American   \n",
      "6         Sahi disha me ja ja raha india ka Future..   \n",
      "7                                         ❤❤❤❤❤❤❤❤❤❤   \n",
      "8                    Love your videos. Thank you ❤❤❤   \n",
      "9  India is  the best and  very beautiful 😍😍😍😍😍😍😍...   \n",
      "\n",
      "                                        cleaned_text  prediction  \\\n",
      "0  please lesbian flag i beg you \\n\\nyou would ro...           0   \n",
      "1   apply mashed potato juice and mixed it with curd           1   \n",
      "2                            missed calls from mars👽           1   \n",
      "3                                               baaa           1   \n",
      "4    you look like raven from phenomena raven no cap           0   \n",
      "5                                           american           1   \n",
      "6           sahi disha me ja ja raha india ka future           1   \n",
      "7                                         ❤❤❤❤❤❤❤❤❤❤           1   \n",
      "8                     love your videos thank you ❤❤❤           0   \n",
      "9  india is  the best and  very beautiful 😍😍😍😍😍😍😍...           0   \n",
      "\n",
      "   probability_spam  \n",
      "0          0.386405  \n",
      "1          0.819729  \n",
      "2          0.797364  \n",
      "3          0.797364  \n",
      "4          0.455880  \n",
      "5          0.682388  \n",
      "6          0.798697  \n",
      "7          0.797364  \n",
      "8          0.378725  \n",
      "9          0.020555  \n",
      "✅ Predictions saved to comments_with_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the trained model and vectorizer\n",
    "with open(\"rf_model.pkl\", \"rb\") as f:\n",
    "    rf_model = pickle.load(f)\n",
    "\n",
    "with open(\"vectorizer.pkl\", \"rb\") as f:\n",
    "    tfidf_vectorizer = pickle.load(f)\n",
    "\n",
    "# Transform the cleaned comments using the same TF-IDF vectorizer\n",
    "X_comments = tfidf_vectorizer.transform(comments[\"cleaned_text\"])\n",
    "\n",
    "# Apply model to predict\n",
    "comments[\"prediction\"] = rf_model.predict(X_comments)\n",
    "\n",
    "# If you want probability scores too\n",
    "comments[\"probability_spam\"] = rf_model.predict_proba(X_comments)[:, 1]\n",
    "\n",
    "# Preview results\n",
    "print(comments[[\"textOriginal\", \"cleaned_text\", \"prediction\", \"probability_spam\"]].head(10))\n",
    "\n",
    "# Save results to CSV\n",
    "comments.to_csv(\"comments_with_predictions.csv\", index=False)\n",
    "print(\"✅ Predictions saved to comments_with_predictions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81083e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction\n",
      "1    3158459\n",
      "0    1566296\n",
      "Name: count, dtype: int64\n",
      "prediction\n",
      "0    0.244680\n",
      "1    0.755141\n",
      "Name: probability_spam, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(comments['prediction'].value_counts())\n",
    "print(comments.groupby('prediction')['probability_spam'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "92e69125",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-01 15:53:00.851 WARNING streamlit.runtime.state.session_state_proxy: Session state does not function when running a script without `streamlit run`\n",
      "2025-09-01 15:53:01.017 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run c:\\Users\\Acer\\anaconda3\\new\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "\n",
    "user_input = st.text_area(\"Enter a comment:\")\n",
    "if st.button(\"Check Spam\"):\n",
    "    X = vectorizer.transform([user_input])\n",
    "    prob = model.predict_proba(X)[0,1]\n",
    "    st.write(\"Spam probability:\", round(prob,2))\n",
    "    st.write(\"Prediction:\", \"Spam\" if prob > 0.5 else \"Not Spam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9698a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
